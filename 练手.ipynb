{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf96821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ppo_loss(pi_new, pi_old, v_now, r, v_next, eps=0.2, c1=0.5, c2=0.01, gamma=0.99):\n",
    "    \"\"\"\n",
    "    PPO核心损失计算（变量极简+逻辑清晰）\n",
    "    输入：\n",
    "        pi_new: 新策略输出的动作log概率 (batch_size,)\n",
    "        pi_old: 旧策略输出的动作log概率 (batch_size,)\n",
    "        v_now: 当前状态价值估计 (batch_size,)\n",
    "        r: 即时奖励 (batch_size,)\n",
    "        v_next: 下一个状态价值估计 (batch_size,)\n",
    "        eps: 裁剪系数（默认0.2）\n",
    "        c1: 价值损失权重（默认0.5）\n",
    "        c2: 熵正则权重（默认0.01）\n",
    "        gamma: 折扣因子（默认0.99）\n",
    "    输出：\n",
    "        total_loss: PPO总损失\n",
    "    \"\"\"\n",
    "    # 1. 计算优势值A（TD误差近似，也可替换为GAE）\n",
    "    td_target = r + gamma * v_next  # TD目标（真实长期回报近似）\n",
    "    A = td_target - v_now           # 优势值（动作好坏的衡量）\n",
    "    \n",
    "    # 2. 策略损失（带裁剪的重要性采样）\n",
    "    ratio = torch.exp(pi_new - pi_old)  # 重要性比：pi_new/pi_old（log转exp）\n",
    "    clip_ratio = torch.clamp(ratio, 1-eps, 1+eps)  # 裁剪到[0.8,1.2]\n",
    "    surr1 = ratio * A                  # 未裁剪项\n",
    "    surr2 = clip_ratio * A             # 裁剪项\n",
    "    policy_loss = -torch.mean(torch.min(surr1, surr2))  # 取min+负号（梯度下降）\n",
    "    \n",
    "    # 3. 价值损失（MSE拟合TD目标）\n",
    "    value_loss = c1 * F.mse_loss(v_now, td_target)\n",
    "    \n",
    "    # 4. 熵正则（鼓励探索，可选但推荐）\n",
    "    entropy = -torch.mean(pi_new)  # 策略熵（log概率的负期望，熵越大探索性越强）\n",
    "    entropy_loss = -c2 * entropy   # 负号：最小化损失=最大化熵\n",
    "    \n",
    "    # 5. 总损失\n",
    "    total_loss = policy_loss + value_loss + entropy_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: yewang0628 wangye374127@gmail.com\n",
    "Date: 2025-11-13 13:50:38\n",
    "LastEditors: yewang0628 wangye374127@gmail.com\n",
    "LastEditTime: 2025-11-13 14:17:13\n",
    "FilePath: \\vs_code\\练手.ipynb\n",
    "Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "'''\n",
    "# PPO GAE版\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gae_advantage(v_now, v_next, r, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    计算GAE（广义优势估计），单独抽离函数，逻辑更清晰\n",
    "    输入：\n",
    "        v_now: 当前状态价值 (batch_size,) 或 (traj_len,)\n",
    "        v_next: 下一个状态价值 (batch_size,) 或 (traj_len,)\n",
    "        r: 即时奖励 (batch_size,) 或 (traj_len,)\n",
    "        gamma: 折扣因子（与主函数一致）\n",
    "        gae_lambda: GAE参数（默认0.95，平衡偏差-方差）\n",
    "    输出：\n",
    "        A: GAE优势值 (batch_size,) 或 (traj_len,)\n",
    "    \"\"\"\n",
    "    # 1. 计算时序差分残差（TD Error）\n",
    "    td_residual = r + gamma * v_next - v_now  # δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    \n",
    "    # 2. 反向累加计算GAE（从最后一个时间步往前推）\n",
    "    A = torch.zeros_like(td_residual)\n",
    "    advantage = 0.0  # 初始优势值（最后一个时间步之后无后续，优势为0）\n",
    "    # 反向遍历：从后往前累加 (γλ)^k * δ_{t+k}\n",
    "    for t in reversed(range(len(td_residual))):\n",
    "        advantage = td_residual[t] + gamma * gae_lambda * advantage\n",
    "        A[t] = advantage\n",
    "    \n",
    "    # 可选：优势值标准化（减少训练波动，推荐添加）\n",
    "    A = (A - A.mean()) / (A.std() + 1e-8)  # 加1e-8避免除零\n",
    "    return A\n",
    "\n",
    "def ppo_loss(pi_new, pi_old, v_now, r, v_next, eps=0.2, c1=0.5, c2=0.01, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    PPO核心损失计算（GAE优势值版，变量极简+逻辑清晰）\n",
    "    输入新增：\n",
    "        gae_lambda: GAE参数（默认0.95，工业界常用值）\n",
    "    其他输入/输出与原版本一致\n",
    "    \"\"\"\n",
    "    # 1. 计算GAE优势值（替换原TD误差近似）\n",
    "    A = gae_advantage(v_now, v_next, r, gamma, gae_lambda)\n",
    "    \n",
    "    # 2. 策略损失（带裁剪的重要性采样，逻辑不变）\n",
    "    ratio = torch.exp(pi_new - pi_old)  # 重要性比：pi_new/pi_old（log转exp，数值稳定）\n",
    "    clip_ratio = torch.clamp(ratio, 1-eps, 1+eps)  # 裁剪到[0.8,1.2]\n",
    "    surr1 = ratio * A\n",
    "    surr2 = clip_ratio * A\n",
    "    policy_loss = -torch.mean(torch.min(surr1, surr2))  # 取min+负号（梯度下降适配）\n",
    "    \n",
    "    # 3. 价值损失（MSE拟合TD目标，逻辑不变）\n",
    "    td_target = r + gamma * v_next  # TD目标依然用于价值网络拟合\n",
    "    value_loss = c1 * F.mse_loss(v_now, td_target)\n",
    "    \n",
    "    # 4. 熵正则（鼓励探索，逻辑不变）\n",
    "    entropy = -torch.mean(pi_new)  # 离散动作策略熵（log概率的负期望）\n",
    "    entropy_loss = -c2 * entropy\n",
    "    \n",
    "    # 5. 总损失\n",
    "    total_loss = policy_loss + value_loss + entropy_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
