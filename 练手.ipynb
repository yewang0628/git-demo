{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf96821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ppo_loss(pi_new, pi_old, v_now, r, v_next, eps=0.2, c1=0.5, c2=0.01, gamma=0.99):\n",
    "    \"\"\"\n",
    "    PPO核心损失计算（变量极简+逻辑清晰）\n",
    "    输入：\n",
    "        pi_new: 新策略输出的动作log概率 (batch_size,)\n",
    "        pi_old: 旧策略输出的动作log概率 (batch_size,)\n",
    "        v_now: 当前状态价值估计 (batch_size,)\n",
    "        r: 即时奖励 (batch_size,)\n",
    "        v_next: 下一个状态价值估计 (batch_size,)\n",
    "        eps: 裁剪系数（默认0.2）\n",
    "        c1: 价值损失权重（默认0.5）\n",
    "        c2: 熵正则权重（默认0.01）\n",
    "        gamma: 折扣因子（默认0.99）\n",
    "    输出：\n",
    "        total_loss: PPO总损失\n",
    "    \"\"\"\n",
    "    # 1. 计算优势值A（TD误差近似，也可替换为GAE）\n",
    "    td_target = r + gamma * v_next  # TD目标（真实长期回报近似）\n",
    "    A = td_target - v_now           # 优势值（动作好坏的衡量）\n",
    "    \n",
    "    # 2. 策略损失（带裁剪的重要性采样）\n",
    "    ratio = torch.exp(pi_new - pi_old)  # 重要性比：pi_new/pi_old（log转exp）\n",
    "    clip_ratio = torch.clamp(ratio, 1-eps, 1+eps)  # 裁剪到[0.8,1.2]\n",
    "    surr1 = ratio * A                  # 未裁剪项\n",
    "    surr2 = clip_ratio * A             # 裁剪项\n",
    "    policy_loss = -torch.mean(torch.min(surr1, surr2))  # 取min+负号（梯度下降）\n",
    "    \n",
    "    # 3. 价值损失（MSE拟合TD目标）\n",
    "    value_loss = c1 * F.mse_loss(v_now, td_target)\n",
    "    \n",
    "    # 4. 熵正则（鼓励探索，可选但推荐）\n",
    "    entropy = -torch.mean(pi_new)  # 策略熵（log概率的负期望，熵越大探索性越强）\n",
    "    entropy_loss = -c2 * entropy   # 负号：最小化损失=最大化熵\n",
    "    \n",
    "    # 5. 总损失\n",
    "    total_loss = policy_loss + value_loss + entropy_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: yewang0628 wangye374127@gmail.com\n",
    "Date: 2025-11-13 13:50:38\n",
    "LastEditors: yewang0628 wangye374127@gmail.com\n",
    "LastEditTime: 2025-11-13 14:17:13\n",
    "FilePath: \\vs_code\\练手.ipynb\n",
    "Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "'''\n",
    "# PPO GAE版\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gae_advantage(v_now, v_next, r, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    计算GAE（广义优势估计），单独抽离函数，逻辑更清晰\n",
    "    输入：\n",
    "        v_now: 当前状态价值 (batch_size,) 或 (traj_len,)\n",
    "        v_next: 下一个状态价值 (batch_size,) 或 (traj_len,)\n",
    "        r: 即时奖励 (batch_size,) 或 (traj_len,)\n",
    "        gamma: 折扣因子（与主函数一致）\n",
    "        gae_lambda: GAE参数（默认0.95，平衡偏差-方差）\n",
    "    输出：\n",
    "        A: GAE优势值 (batch_size,) 或 (traj_len,)\n",
    "    \"\"\"\n",
    "    # 1. 计算时序差分残差（TD Error）\n",
    "    td_residual = r + gamma * v_next - v_now  # δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    \n",
    "    # 2. 反向累加计算GAE（从最后一个时间步往前推）\n",
    "    A = torch.zeros_like(td_residual)\n",
    "    advantage = 0.0  # 初始优势值（最后一个时间步之后无后续，优势为0）\n",
    "    # 反向遍历：从后往前累加 (γλ)^k * δ_{t+k}\n",
    "    for t in reversed(range(len(td_residual))):\n",
    "        advantage = td_residual[t] + gamma * gae_lambda * advantage\n",
    "        A[t] = advantage\n",
    "    \n",
    "    # 可选：优势值标准化（减少训练波动，推荐添加）\n",
    "    A = (A - A.mean()) / (A.std() + 1e-8)  # 加1e-8避免除零\n",
    "    return A\n",
    "\n",
    "def ppo_loss(pi_new, pi_old, v_now, r, v_next, eps=0.2, c1=0.5, c2=0.01, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    PPO核心损失计算（GAE优势值版，变量极简+逻辑清晰）\n",
    "    输入新增：\n",
    "        gae_lambda: GAE参数（默认0.95，工业界常用值）\n",
    "    其他输入/输出与原版本一致\n",
    "    \"\"\"\n",
    "    # 1. 计算GAE优势值（替换原TD误差近似）\n",
    "    A = gae_advantage(v_now, v_next, r, gamma, gae_lambda)\n",
    "    \n",
    "    # 2. 策略损失（带裁剪的重要性采样，逻辑不变）\n",
    "    ratio = torch.exp(pi_new - pi_old)  # 重要性比：pi_new/pi_old（log转exp，数值稳定）\n",
    "    clip_ratio = torch.clamp(ratio, 1-eps, 1+eps)  # 裁剪到[0.8,1.2]\n",
    "    surr1 = ratio * A\n",
    "    surr2 = clip_ratio * A\n",
    "    policy_loss = -torch.mean(torch.min(surr1, surr2))  # 取min+负号（梯度下降适配）\n",
    "    \n",
    "    # 3. 价值损失（MSE拟合TD目标，逻辑不变）\n",
    "    td_target = r + gamma * v_next  # TD目标依然用于价值网络拟合\n",
    "    value_loss = c1 * F.mse_loss(v_now, td_target)\n",
    "    \n",
    "    # 4. 熵正则（鼓励探索，逻辑不变）\n",
    "    entropy = -torch.mean(pi_new)  # 离散动作策略熵（log概率的负期望）\n",
    "    entropy_loss = -c2 * entropy\n",
    "    \n",
    "    # 5. 总损失\n",
    "    total_loss = policy_loss + value_loss + entropy_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21236eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DPOloss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dpo_loss(pi_w, pi_l, beta=0.1):\n",
    "    \"\"\"\n",
    "    DPO 核心损失（变量极简版）\n",
    "    输入：\n",
    "        pi_w: 新策略对「赢样本w」的log概率 (batch_size,) —— 人类偏好的回复\n",
    "        pi_l: 新策略对「输样本l」的log概率 (batch_size,) —— 人类不偏好的回复\n",
    "        beta: 温度超参（控制偏好强度，默认0.1，工业界常用）\n",
    "    输出：\n",
    "        dpo_total_loss: DPO总损失\n",
    "    \"\"\"\n",
    "    # 1. 计算偏好加权后的log概率（beta放大偏好差异）\n",
    "    w_score = beta * pi_w\n",
    "    l_score = beta * pi_l\n",
    "    \n",
    "    # 2. 对比损失：让赢样本的相对概率最大化（交叉熵形式）\n",
    "    # 等价于 -log( exp(w_score) / (exp(w_score) + exp(l_score)) )\n",
    "    logits = torch.stack([w_score, l_score], dim=1)  # (batch, 2)\n",
    "    loss = -torch.mean(F.log_softmax(logits, dim=1)[:, 0])  # 只关注赢样本的softmax概率\n",
    "    \n",
    "    # 3. 可选：熵正则（鼓励探索，可选添加）\n",
    "    entropy = -torch.mean(pi_w)  # 用赢样本的熵近似（简化处理）\n",
    "    entropy_loss = 0.01 * entropy  # 权重0.01，弱正则\n",
    "    \n",
    "    return loss - entropy_loss  # 负号：最小化损失=最大化熵\n",
    "\n",
    "if __name__=='__mian__':\n",
    "    # 模拟batch_size=32的偏好数据（赢/输样本的log概率）\n",
    "    batch_size = 32\n",
    "    pi_w = torch.randn(batch_size)  # 赢样本log概率\n",
    "    pi_l = torch.randn(batch_size)  # 输样本log概率\n",
    "\n",
    "    loss = dpo_loss(pi_w, pi_l)\n",
    "    print(\"DPO损失:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRPO loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def grpo_loss(pi_new, pi_old, r, group_size=4, eps=0.2, c2=0.01):\n",
    "    \"\"\"\n",
    "    GRPO 核心损失（变量极简版，无Critic，组内对比估算优势）\n",
    "    输入：\n",
    "        pi_new: 新策略的log概率 (batch_size × group_size,) —— 组内所有样本的logp\n",
    "        pi_old: 旧策略的log概率 (batch_size × group_size,) —— 固定不更新\n",
    "        r: 组内样本的奖励/偏好分数 (batch_size × group_size,) —— 组内每条回复的得分\n",
    "        group_size: 每组采样数量（默认4，常用值）\n",
    "        eps: 裁剪系数（同PPO，默认0.2）\n",
    "        c2: 熵正则权重（默认0.01）\n",
    "    输出：\n",
    "        grpo_total_loss: GRPO总损失\n",
    "    \"\"\"\n",
    "    # 1. 重构维度：适应组内计算 (batch_size, group_size)\n",
    "    batch_size = pi_new.shape[0] // group_size\n",
    "    pi_new = pi_new.reshape(batch_size, group_size)\n",
    "    pi_old = pi_old.reshape(batch_size, group_size)\n",
    "    r = r.reshape(batch_size, group_size)\n",
    "    \n",
    "    # 2. 组内相对优势估算（替代Critic，核心！）\n",
    "    # 优势 = 样本奖励 - 组内平均奖励（组内对比，体现相对好坏）\n",
    "    r_mean = r.mean(dim=1, keepdim=True)  # 每组的平均奖励\n",
    "    A = r - r_mean  # 组内相对优势（无需Critic，直接通过组内对比获得）\n",
    "    \n",
    "    # 3. 策略损失（带裁剪的重要性采样，同PPO核心逻辑）\n",
    "    ratio = torch.exp(pi_new - pi_old)  # 重要性比（log转exp，数值稳定）\n",
    "    clip_ratio = torch.clamp(ratio, 1-eps, 1+eps)  # 限制更新幅度\n",
    "    surr1 = ratio * A\n",
    "    surr2 = clip_ratio * A\n",
    "    policy_loss = -torch.mean(torch.min(surr1, surr2))  # 取min+负号（梯度下降）\n",
    "    \n",
    "    # 4. 熵正则（鼓励组内多样性，可选但推荐）\n",
    "    entropy = -torch.mean(pi_new, dim=1)  # 每组的熵\n",
    "    entropy_loss = -c2 * entropy.mean()  # 批量平均\n",
    "    \n",
    "    # 5. 总损失（无价值损失，因为去除了Critic）\n",
    "    total_loss = policy_loss + entropy_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "if __name__=='__main__':\n",
    "# 模拟输入：batch_size=8组，每组4个样本（total=32个样本）\n",
    "    batch_size = 8\n",
    "    group_size = 4\n",
    "    total_samples = batch_size * group_size\n",
    "\n",
    "    pi_new = torch.randn(total_samples)  # 新策略log概率\n",
    "    pi_old = torch.randn(total_samples)  # 旧策略log概率（固定）\n",
    "    r = torch.randn(total_samples)       # 组内样本奖励（可正可负）\n",
    "\n",
    "    loss = grpo_loss(pi_new, pi_old, r, group_size=group_size)\n",
    "    print(\"GRPO总损失:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ac8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def grpo_loss(pi_new, pi_old, r, group_size=4, eps=0.2, c2=0.01, std_eps=1e-8):\n",
    "    \"\"\"\n",
    "    GRPO 核心损失（带标准差归一化的优势计算）\n",
    "    输入：\n",
    "        pi_new: 新策略的log概率 (batch_size × group_size,) —— 组内所有样本的logp\n",
    "        pi_old: 旧策略的log概率 (batch_size × group_size,) —— 固定不更新\n",
    "        r: 组内样本的奖励/偏好分数 (batch_size × group_size,) —— 组内每条回复的得分\n",
    "        group_size: 每组采样数量（默认4，常用值）\n",
    "        eps: 裁剪系数（同PPO，默认0.2）\n",
    "        c2: 熵正则权重（默认0.01）\n",
    "        std_eps: 防止除以零的小常数（默认1e-8）\n",
    "    输出：\n",
    "        grpo_total_loss: GRPO总损失\n",
    "    \"\"\"\n",
    "    # 1. 重构维度：适应组内计算 (batch_size, group_size)\n",
    "    batch_size = pi_new.shape[0] // group_size\n",
    "    pi_new = pi_new.reshape(batch_size, group_size)\n",
    "    pi_old = pi_old.reshape(batch_size, group_size)\n",
    "    r = r.reshape(batch_size, group_size)\n",
    "    \n",
    "    # 2. 组内相对优势估算（带标准差归一化）\n",
    "    r_mean = r.mean(dim=1, keepdim=True)  # 每组的平均奖励\n",
    "    r_std = r.std(dim=1, keepdim=True)    # 每组的奖励标准差\n",
    "    # 优势 = (样本奖励 - 组内平均奖励) / 组内标准差（标准化处理）\n",
    "    A = (r - r_mean) / (r_std + std_eps)  # 加入std_eps防止除以零\n",
    "    \n",
    "    # 3. 策略损失（带裁剪的重要性采样，同PPO核心逻辑）\n",
    "    ratio = torch.exp(pi_new - pi_old)  # 重要性比（log转exp，数值稳定）\n",
    "    clip_ratio = torch.clamp(ratio, 1-eps, 1+eps)  # 限制更新幅度\n",
    "    surr1 = ratio * A\n",
    "    surr2 = clip_ratio * A\n",
    "    policy_loss = -torch.mean(torch.min(surr1, surr2))  # 取min+负号（梯度下降）\n",
    "    \n",
    "    # 4. 熵正则（鼓励组内多样性，可选但推荐）\n",
    "    entropy = -torch.mean(pi_new, dim=1)  # 每组的熵\n",
    "    entropy_loss = -c2 * entropy.mean()  # 批量平均\n",
    "    \n",
    "    # 5. 总损失（无价值损失，因为去除了Critic）\n",
    "    total_loss = policy_loss + entropy_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # 模拟输入：batch_size=8组，每组4个样本（total=32个样本）\n",
    "    batch_size = 8\n",
    "    group_size = 4\n",
    "    total_samples = batch_size * group_size\n",
    "\n",
    "    pi_new = torch.randn(total_samples)  # 新策略log概率\n",
    "    pi_old = torch.randn(total_samples)  # 旧策略log概率（固定）\n",
    "    r = torch.randn(total_samples)       # 组内样本奖励（可正可负）\n",
    "\n",
    "    loss = grpo_loss(pi_new, pi_old, r, group_size=group_size)\n",
    "    print(\"带标准差归一化的GRPO总损失:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9187fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
